{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Visualization of Complex Agro-Environmental Data\n",
    "---\n",
    "## Checking probability distributions\n",
    "### Normality tests\n",
    "\n",
    "To follow the most appropriate statistical methods (e.g. hypothesis testing or statistical modelling approach), it is often important to test whether variables can be assumed to follow a normal distibution. If the used methods assume a Gaussian distribution but your data was drawn from a different distribution, the findings may be misleading or simply wrong.\n",
    "\n",
    "The first commonly way of quickly checking the distribution of a sample of data is to plot a histogram.\n",
    "However, a more informative visualization is to plot a normal QQ plot, that plots the quantiles of each data value against theoretical quantiles corresponding to equal probability intervals of the standard normal distribution *N*(0,1). If most points fall near the diagonal line of the plot, then the distribution of data approximates a normal distribution.\n",
    "\n",
    "A more formal way of checking if a sample of data was drawn from a normal distribution is to run statistical normality tests. There are several alternative normality tests, each making different assumptions and considering different aspects of the data.\n",
    "\n",
    "#### Normal Q-Q plots\n",
    "\n",
    "The python module statsmodels provides the qqplot funtion to plot QQ plots. It can be applied to any distribution but by default assumes we are comparing our data to a Gaussian distribution. \n",
    "\n",
    "Note that theoretical quantiles (x-axis) get closer to each other towards the mean (=0.0, in this case) - this is because the quantile intervals that represent equal probability intervals (given by the area under the density probability curve) are lower near the mean than in both extremes of the distribution curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # For plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class exercise (9 numbers selected by students between 1 and 10)\n",
    "\n",
    "data=np.array([2,2,4,5,6,7,7,8,9,10])\n",
    "qqplot(data, line='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of sample size on qqplots (samples taken from a population with a uniform distribution)\n",
    "\n",
    "data1 = np.random.uniform(1,10, size=9) # observations taken from a population with an uniform distribution\n",
    "data2 = np.random.uniform(1,10, size=99) # observations taken from a population with an uniform distribution\n",
    "data3 = np.random.uniform(1,10, size=999) # observations taken from a population with an uniform distribution\n",
    "\n",
    "qqplot(data1, line='s')\n",
    "qqplot(data2, line='s')\n",
    "qqplot(data3, line='s')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of sample size on qqplots (samples taken from a normal distributed population)\n",
    "\n",
    "data1 = randn(9) # observations taken from a population with an uniform distribution\n",
    "data2 = randn(99) # observations taken from a population with an uniform distribution\n",
    "data3 = randn(999) # observations taken from a population with an uniform distribution\n",
    "\n",
    "qqplot(data1, line='s')\n",
    "qqplot(data2, line='s')\n",
    "qqplot(data3, line='s')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated data with normal distribution\n",
    "\n",
    "# seed the random number generator\n",
    "seed(1)\n",
    "# generate univariate observations\n",
    "data = randn(50)\n",
    "# Histogram\n",
    "sns.histplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q-q plot\n",
    "qqplot(data, line='s')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality hypothesis testing\n",
    "\n",
    "#### Shapiro-Wilk Test\n",
    "The Shapiro-Wilk test quantifies how likely a data sample was drawn from a Gaussian distribution. Considered to be a reliable test of normality, but especially for smaller samples of data (e.g. thousands of observations or less).\n",
    "\n",
    "This test is implemented in the shapiro() function of the SciPy module. The function returns both the W-statistic calculated by the test and the p-value.\n",
    "\n",
    "Check also: help(shapiro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import function\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# normality test\n",
    "stat, p = shapiro(data)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p)) # print outputs with 3 decimal places\n",
    "# interpret. H0: 'the sample was drawn from a Gaussian distribution'.\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    " print('Sample is not significantly different from Gaussian (fail to reject H0. Rejecting H0 has an error probability >0.05)')\n",
    "else:\n",
    " print('Sample is significantly different from Gaussian (reject H0 with an error probability <0.05)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D’Agostino’s K<sup>2</sup> test \n",
    "\n",
    "Calculates kurtosis and skewness from the data to determine if the data distribution departs from the normal distribution.\n",
    "\n",
    "The D’Agostino’s K<sup>2</sup> test is available via the normaltest() SciPy function and returns the test statistic and the p-value.\n",
    "\n",
    "Check also: help(normaltest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "\n",
    "# normality test\n",
    "stat, p = normaltest(data)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p)) # print outputs\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    " print('Sample is not significantly different from Gaussian (fail to reject H0. Rejecting H0 has an error probability >0.05)')\n",
    "else:\n",
    " print('Sample is significantly different from Gaussian (reject H0 with an error probability <0.05)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kolmogorov-Smirnov normality test\n",
    "\n",
    "This is a non-parametric test i.e., it has no assumption about the distribution of the data. Kolmogorov-Smirnov test is used to understand how well the distribution of sample data conforms to some theoretical distribution. \n",
    "\n",
    "In this, we compare between some theoretical cumulative distribution function, (Ft(x)), and a samples’ cumulative distribution function , (Fs(x)) where the sample is a random sample with unknown cumulative distribution function Fs(x):\n",
    "\n",
    "H0: Fs(x) is equal to Ft(x) for all x from -inf. to inf.\n",
    "\n",
    "HA: Fs(x) is not equal to Ft(x) for at least one x\n",
    "\n",
    "The K-S test is implemented in the kstest() function of SciPy. In kstest function, the parameter “alternative” is used for the alternative hypothesis with default value of “two-sided” and for “args”, ‘norm’ option is given to compare. Again, we can conclude, all the variables are not normally distributed as p-value is 0 in all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "\n",
    "# K-S test\n",
    "stat, p = kstest(data, 'norm')\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p)) # print outputs\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    " print('Sample is not significantly different from Gaussian (fail to reject H0. Rejecting H0 has an error probability >0.05)')\n",
    "else:\n",
    " print('Sample is significantly different from Gaussian (reject H0 with an error probability <0.05)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anderson-Darling Test\n",
    "\n",
    "A statistical test that can be used to evaluate whether a data sample comes from one of among many known data samples (not only from normal distribution). The Anderson Darling test is one of the most powerful normality tests because it is less sensitive to outliers than other tests. However, it has a higher Type I error rate (rejection of the null hypothesis when it is actually true) than other normality tests.\n",
    "\n",
    "It can be used to check whether a data sample comes from a normal distribution. The test is a modified version of the Kolmogorov-Smirnov test (K-S test). It gives more weight to the tails of the distribution than does the K-S test. \n",
    "\n",
    "The Anderson-Darling test returns a list of critical values rather than a single p-value. These correspond to percentage points of the distribution under the null hypothesis (for normal distribution: 15%, 10%, 5%, 2.5%, 1%). This can provide the basis for a more thorough interpretation of the result.\n",
    "\n",
    "The Anderson-Darling test is implemented in SciPy in the anderson() function. It takes as parameters the data sample and the name of the distribution to test it against. By default, the test will check against the Gaussian distribution (dist=’norm’).\n",
    "\n",
    "Check also: help(anderson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import anderson\n",
    "\n",
    "# normality test\n",
    "result = anderson(data)\n",
    "\n",
    "print('Statistic: %.3f' % result.statistic)\n",
    "print('critical values:', result.critical_values)\n",
    "print('significance level (%):', result.significance_level)\n",
    "print('Fit results:', result.fit_result)\n",
    "\n",
    "p = 0\n",
    "for i in range(len(result.critical_values)):\n",
    "    slevel, cvalues = result.significance_level[i], result.critical_values[i]\n",
    "    if result.statistic < result.critical_values[i]:\n",
    "        print('%.3f: %.3f, data looks normal (fail to reject H0)' % (slevel, cvalues))\n",
    "    else:\n",
    "        print('%.3f: %.3f, data does not look normal (reject H0)' % (slevel, cvalues))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/\n",
    "\n",
    "https://towardsdatascience.com/methods-for-normality-test-with-application-in-python-bb91b49ed0f5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Visualization of Complex Agro-Environmental Data\n",
    "---\n",
    "## Ordination Analysis\n",
    "\n",
    "### 1. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is one of the most popular methods in Exploratory Data Analysis, as a means to explore, through visualizations, the relationships among a set of objects (sampling units, indiv√≠duals, experimental plots, ...) in relation to a set of variables (different measurements undertaken at each object). It is also often use to create few orthogonal latent variables that retain most of the original data variance. These latent variables are specially usefull in regression and machine learning, to avoid problems derived from multicolinearity among explanatory variables.\n",
    "\n",
    "In `python` PCA is implemented in the `sklearn.cluster` module, in the `sklearn.decomposition.PCA` function (check [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html))\n",
    "\n",
    "### 1.1 Data preparation and exploration\n",
    "\n",
    "To run the analysis you first need to import necessary modules and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyse the dataset `winequality_red.csv` available [here](https://www.kaggle.com/datasets/sgus1318/winedata?resource=download) with 12 variables (columns) related with wine quality characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('winequality_red.csv')\n",
    "df_wine.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_wine()['quality'],"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove variable `quality`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_wine.iloc[:, 0:11]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check multicolinearity in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrdot(*args, **kwargs):\n",
    "    corr_r = args[0].corr(args[1], 'pearson')\n",
    "    corr_text = f\"{corr_r:2.2f}\".replace(\"0.\", \".\")\n",
    "    ax = plt.gca()\n",
    "    ax.set_axis_off()\n",
    "    marker_size = abs(corr_r) * 10000\n",
    "    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap=\"coolwarm\",\n",
    "               vmin=-1, vmax=1, transform=ax.transAxes)\n",
    "    font_size = abs(corr_r) * 40 + 5\n",
    "    ax.annotate(corr_text, [.5, .5,],  xycoords=\"axes fraction\",\n",
    "                ha='center', va='center', fontsize=font_size)\n",
    "\n",
    "sns.set(style='white', font_scale=1.6)\n",
    "\n",
    "g = sns.PairGrid(df, aspect=1.4, diag_sharey=False)\n",
    "g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})\n",
    "g.map_diag(sns.histplot, kde_kws={'color': 'black'})\n",
    "g.map_upper(corrdot);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a few exceptions, variables are not so correlated with each other. This means that probably it will be difficult that the first Principal Components will retain most of the variance in the data.\n",
    "\n",
    "### 1.2 Data standardization\n",
    "\n",
    "Now we need to scale the variables before conducting the analysis to avoid misleading PCA results due to units differences. The variables need to be standardize, so that the mean of each variable = 0 and the variance = 1. We will use the `StandardScaler` class of `scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wine_scaled = StandardScaler().fit_transform(df)\n",
    "# As a result, we obtained a two-dimensional NumPy array. We can convert it to a pandas DataFrame for a better display.\n",
    "df_scaled = pd.DataFrame(data=wine_scaled, \n",
    "                                columns=df.columns)\n",
    "df_scaled.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run PCA.\n",
    "\n",
    "### 1.3 Select the number of Principal Components (PC)\n",
    "\n",
    "To select the number of PC, usually a Scree plot is produced to visualize the percentage of explained variance or the eigenvalues per component. Either the `elbow rule` - retaining all components before the point where the curve flattens out - or the `Kaiser's rule` - retaining components whose eigenvalues are greater than 1.\n",
    "\n",
    "First we run PCA for 11 components (number of original variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=11)\n",
    "pca.fit_transform(df_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have run the PCA and now we can extract the proportion of explained variance and the eigenvalues as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = pca.explained_variance_ # eigenvalues\n",
    "prop_var = pca.explained_variance_ratio_ # proportion of explained variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the scree plot with proportion of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_numbers = np.arange(pca.n_components_) + 1\n",
    " \n",
    "plt.plot(PC_numbers, \n",
    "         prop_var,\n",
    "         'ro-')\n",
    "plt.title('Scree Plot', fontsize=20)\n",
    "plt.ylabel('Proportion of Variance', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Elbow rule` is not easy to apply to our data.\n",
    "We can try to apply the `Kaiser's rule` and for that we need to plot the eigenvalues instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_numbers = np.arange(pca.n_components_) + 1\n",
    " \n",
    "plt.plot(PC_numbers, \n",
    "         eigenvalues,\n",
    "         'ro-')\n",
    "plt.title('Scree Plot', fontsize=20)\n",
    "plt.ylabel('Eigenvalues', fontsize=16)\n",
    "plt.axhline(y=1, color='r', \n",
    "            linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this rule, 4 components should be retained. However, just for the purpose of ilustrating how to produce a PCA biplot vlisualization, we will retain only two components.\n",
    "\n",
    "### 1.4 Visualize and interpret the PCA biplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "PC = pca.fit_transform(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_wine = pd.DataFrame(data = PC, \n",
    "                            columns = ['PC1', 'PC2'])\n",
    "pca_wine.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"score\" - Scores of each object (for each PC) - PC\n",
    "# \"coef\" - PCA variable loadings (for each PC) - pca.components_\n",
    "# labels - name of variables - list(df.columns)\n",
    "\n",
    "\n",
    "def biplot(score,coef,labels=None): \n",
    " \n",
    "    xs = score[:,0] # PC1 object scores\n",
    "    ys = score[:,1] # PC2 object scores \n",
    "    n = coef.shape[0] # number of dimensions (2)\n",
    "    scalex = 1.0/(xs.max() - xs.min()) # to rescale scores\n",
    "    scaley = 1.0/(ys.max() - ys.min()) # to rescale scores\n",
    "    plt.scatter(xs * scalex,ys * scaley,\n",
    "                s=6, \n",
    "                color='blue') # scatter plot using rescaled object scores\n",
    " \n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coef[i,0], \n",
    "                  coef[i,1],color = 'red',\n",
    "                  head_width=0.01,\n",
    "                  alpha = 0.5) # plot arrows for each variable\n",
    "        plt.text(coef[i,0]* 1.15, \n",
    "                 coef[i,1] * 1.15, \n",
    "                 labels[i], \n",
    "                 color = 'red', \n",
    "                 ha = 'center', \n",
    "                 va = 'center') # variable labels for each arrow\n",
    " \n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(2))    \n",
    " \n",
    " \n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Biplot of PCA')\n",
    " \n",
    "biplot(PC, \n",
    "       np.transpose(pca.components_), \n",
    "       list(df.columns))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same biplot but with colors expressing wine quality\n",
    "\n",
    "PC1 = pca_wine['PC1']/(pca_wine['PC1'].max() - pca_wine['PC1'].min())\n",
    "PC2 = pca_wine['PC2']/(pca_wine['PC2'].max() - pca_wine['PC2'].min())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Biplot of PCA')\n",
    "sns.scatterplot(x=PC1,\n",
    "              y=PC2,\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "\n",
    "n = np.transpose(pca.components_).shape[0] # number of dimensions (2)\n",
    "for i in range(n):\n",
    "        plt.arrow(0, 0, np.transpose(pca.components_)[i,0], \n",
    "                  np.transpose(pca.components_)[i,1], \n",
    "                  color = (0.1, 0.1, 0.1, 0.8),\n",
    "                  head_width=0.02) # plot arrows for each variable\n",
    "        plt.text(np.transpose(pca.components_)[i,0]* 1.15, \n",
    "                 np.transpose(pca.components_)[i,1] * 1.15, \n",
    "                 list(df.columns)[i], \n",
    "                 color = (0.1, 0.1, 0.1, 0.8), \n",
    "                 ha = 'center', \n",
    "                 va = 'center') # variable labels for each arrow\n",
    "plt.legend(title='Wine quality')\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Running PCA from scratch\n",
    "\n",
    "The code that follows computes the same PCA but without depending on any specific PCA function.\n",
    "\n",
    "NOTE: The first step - variable standardization - was already performed\n",
    "\n",
    "Step 2: compute the covariance matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(x): \n",
    "    return (x.T @ x)/(x.shape[0]-1)\n",
    "\n",
    "cov_mat = covariance(df_scaled) # np.cov(X_std.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Compute the eigenvectors and eigenvalues of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import eig\n",
    "\n",
    "# Eigendecomposition of covariance matrix\n",
    "eig_vals, eig_vecs = eig(cov_mat)\n",
    "\n",
    "# Adjusting the eigenvectors (loadings) that are largest in absolute value to be positive\n",
    "max_abs_idx = np.argmax(np.abs(eig_vecs), axis=0)\n",
    "signs = np.sign(eig_vecs[max_abs_idx, range(eig_vecs.shape[0])])\n",
    "eig_vecs = eig_vecs*signs[np.newaxis,:]\n",
    "eig_vecs = eig_vecs.T\n",
    "\n",
    "print('Eigenvalues \\n', eig_vals)\n",
    "print('Eigenvectors \\n', eig_vecs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Rearrange the eigenvectors and eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of eigenvalue and eigenvector tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[i,:]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the tuples from the highest to the lowest eigenvalues\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# For further usage\n",
    "eig_vals_sorted = np.array([x[0] for x in eig_pairs])\n",
    "eig_vecs_sorted = np.array([x[1] for x in eig_pairs])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Select the number of PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top k eigenvectors\n",
    "k = 2\n",
    "W = eig_vecs_sorted[:k, :] # Projection matrix\n",
    "\n",
    "eig_vals_total = sum(eig_vals)\n",
    "explained_variance = [(i / eig_vals_total)*100 for i in eig_vals_sorted]\n",
    "explained_variance = np.round(explained_variance, 2)\n",
    "cum_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print('Explained variance: {}'.format(explained_variance))\n",
    "print('Cumulative explained variance: {}'.format(cum_explained_variance))\n",
    "\n",
    "plt.plot(np.arange(1, 11 + 1), cum_explained_variance, '-o')\n",
    "plt.xticks(np.arange(1,11+1))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Project the data with a biplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "X_proj = df_scaled.dot(W.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create biplot\n",
    "PC1 = X_proj.iloc[:, 0]/(X_proj.iloc[:, 0].max() - X_proj.iloc[:, 0].min())\n",
    "PC2 =X_proj.iloc[:, 1]/(X_proj.iloc[:, 1].max() - X_proj.iloc[:, 1].min())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('2 components, captures {:.1f} of total variation'.format(cum_explained_variance[1]))\n",
    "\n",
    "sns.scatterplot(x=PC1,\n",
    "              y=PC2,\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "\n",
    "n = np.transpose(pca.components_).shape[0] # number of dimensions (2)\n",
    "for i in range(n):\n",
    "        plt.arrow(0, 0, np.transpose(pca.components_)[i,0], \n",
    "                  np.transpose(pca.components_)[i,1], \n",
    "                  color = (0.1, 0.1, 0.1, 0.8),\n",
    "                  head_width=0.02) # plot arrows for each variable\n",
    "        plt.text(np.transpose(pca.components_)[i,0]* 1.15, \n",
    "                 np.transpose(pca.components_)[i,1] * 1.15, \n",
    "                 list(df.columns)[i], \n",
    "                 color = (0.1, 0.1, 0.1, 0.8), \n",
    "                 ha = 'center', \n",
    "                 va = 'center') # variable labels for each arrow\n",
    "plt.legend(title='Wine quality')\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "\n",
    "plt.xlabel('PC1'); plt.xticks([])\n",
    "plt.ylabel('PC2'); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Running PCA based on the correlation matrix\n",
    "\n",
    "Running PCA based on the correlation matrix gives the same result as running a PCA based on the variance-covariance matrix with standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the PCA function, by setting the whiten=True parameter when initializing the PCA object, \n",
    "# the covariance matrix is normalized by dividing each element by the square root of the corresponding product of variances. \n",
    "# This normalization results in a correlation matrix.\n",
    "\n",
    "pca = PCA(n_components=11, whiten=True)\n",
    "PC = pca.fit_transform(df_scaled)\n",
    "pca_wine = pd.DataFrame(data = PC[:,:2], columns = ['PC1', 'PC2'])\n",
    "\n",
    "PC1 = pca_wine['PC1']/(pca_wine['PC1'].max() - pca_wine['PC1'].min())\n",
    "PC2 = pca_wine['PC2']/(pca_wine['PC2'].max() - pca_wine['PC2'].min())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Biplot of PCA')\n",
    "sns.scatterplot(x=PC1,\n",
    "              y=PC2,\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "              \n",
    "n = np.transpose(pca.components_).shape[0] # number of dimensions (2)\n",
    "for i in range(n):\n",
    "        plt.arrow(0, 0, np.transpose(pca.components_)[i,0], \n",
    "                  np.transpose(pca.components_)[i,1], \n",
    "                  color = (0.1, 0.1, 0.1, 0.8),\n",
    "                  head_width=0.02) # plot arrows for each variable\n",
    "        plt.text(np.transpose(pca.components_)[i,0]* 1.15, \n",
    "                 np.transpose(pca.components_)[i,1] * 1.15, \n",
    "                 list(df.columns)[i], \n",
    "                 color = (0.1, 0.1, 0.1, 0.8), \n",
    "                 ha = 'center', \n",
    "                 va = 'center') # variable labels for each arrow\n",
    "plt.legend(title='Wine quality')\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Running Factor Analysis with varimax rotation\n",
    "\n",
    "Varimax rotation allows to reorient the factors so that low loadings become lower and high loadings larger. This facilitates the interpretation of the PC's in terms of their correlation with the variables. In Sklearn, the Varimax is not implemented in `sklearn.decomposition.PCA`, only in `sklearn.decomposition.FactorAnalysis`. Although similar, Factor Analysis is a slightly different approach from PCA. While PCA is a data reduction technique that aims to capture the most variance in the data, FA is a technique for identifying latent factors that underlie the observed correlations among the variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "# Fit factor analysis model\n",
    "favarimax = FactorAnalysis(n_components=2, rotation=\"varimax\")\n",
    "FA = favarimax.fit_transform(df_scaled)\n",
    "\n",
    "fa_wine = pd.DataFrame(data = FA[:,:2], columns = ['F1', 'F2']) # select 2 first array columns from FA\n",
    "\n",
    "F1 = fa_wine['F1']/(fa_wine['F1'].max() - fa_wine['F1'].min())\n",
    "F2 = fa_wine['F2']/(fa_wine['F2'].max() - fa_wine['F2'].min())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Biplot of FA')\n",
    "sns.scatterplot(x=F1,\n",
    "              y=F2,\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "              \n",
    "n = np.transpose(favarimax.components_).shape[0] # number of dimensions (2)\n",
    "for i in range(n):\n",
    "        plt.arrow(0, 0, np.transpose(favarimax.components_)[i,0], \n",
    "                  np.transpose(favarimax.components_)[i,1], \n",
    "                  color = (0.1, 0.1, 0.1, 0.8),\n",
    "                  head_width=0.02) # plot arrows for each variable\n",
    "        plt.text(np.transpose(favarimax.components_)[i,0]* 1.15, \n",
    "                 np.transpose(favarimax.components_)[i,1] * 1.15, \n",
    "                 list(df.columns)[i], \n",
    "                 color = (0.1, 0.1, 0.1, 0.8), \n",
    "                 ha = 'center', \n",
    "                 va = 'center') # variable labels for each arrow\n",
    "plt.legend(title='Wine quality')\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multidimensional Scaling (MDS) and Non-Metric Multidimensional Scaling\n",
    "\n",
    "The aim of this ordination method is to project objects from a higher-dimensional space to a lower-dimensional space while preserving the relative distances between those points as much as possible. PCoA (also known as MDS) can be viewed as a generalization of PCA that allows a much wider range of dissimilarity measures to be used (dissimilarities among objects in PCA are euclidean distances).\n",
    "\n",
    "MDS and NMDS are implemented in the `mds()` function of `sklearn.manifold` module.\n",
    "\n",
    "#### 2.1 Import modules and functions\n",
    "\n",
    "Import necessary modules and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Run MDS\n",
    "\n",
    "We will run MDS using the same wine dataset used in PCA. We will use the standardized data (df_scaled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default the 'mds' argument is set to 'True', which means it will run a MDS\n",
    "# Euclidean distances are also the default \n",
    "# Only two axis are extracted by default\n",
    "mds = MDS(random_state=0, normalized_stress = False) \n",
    "mds_transf = mds.fit_transform(df_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=mds_transf[:,0],\n",
    "              y=mds_transf[:,1],\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may use other distances to compute MDS. For that we need to run the function using a dissimilarity matrix as the input.\n",
    "Let's try running MDS with Manhattan dissimilarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute MDS\n",
    "dist_manhattan = manhattan_distances(df_scaled)\n",
    "mds = MDS(dissimilarity='precomputed', random_state=0, normalized_stress = False)\n",
    "mds_transf2 = mds.fit_transform(dist_manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result in 2D\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=mds_transf2[:,0],\n",
    "              y=mds_transf2[:,1],\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Run NMDS\n",
    "\n",
    "Non-Metric Multidimensional Scaling solved many issues of MDS, namely the computation of the measure used to evaluate how much the distances in the MDS are related with the original dissimilarities. This methods is based on ranks of distances among objects, so the aim is to retain the relative position of objects from each other, not their distances. It is based on a iterative trial & error algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute NMDS\n",
    "nmds = MDS(n_components=5, random_state=0, metric = False, normalized_stress=\"auto\") # 5 components extracted so that stress is > 0.2\n",
    "nmds_transf = nmds.fit_transform(df_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The match between object distance in NMDS and the original dissimilarities is given by the stress, which is proportional to the residuals of that relationship. The stress of an MDS with a perfect match between MDS distances and original dissimilarities will be closer to zero. \n",
    "\n",
    "As a rule of thumb, an NMDS ordination with a stress value around or above 0.2 is deemed suspect and a stress value approaching 0.3 indicates that the ordination is arbitrary. Stress values equal to or below 0.1 are considered fair, while values equal to or below 0.05 indicate good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the stress\n",
    "stress = nmds.stress_\n",
    "print(stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=nmds_transf[:,0],\n",
    "              y=nmds_transf[:,1],\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use the method with dissimilarities other than Euclidean distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMDS based on Manhattan distances\n",
    "nmds2 = MDS(metric = False, random_state=0, normalized_stress = 'auto', dissimilarity='precomputed')\n",
    "nmds_transf2 = nmds2.fit_transform(dist_manhattan)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=nmds_transf2[:,0],\n",
    "              y=nmds_transf2[:,1],\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress = nmds2.stress_\n",
    "print(stress)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Discriminant Analysis\n",
    "\n",
    "Discriminant Analysis (DA) is used when we have observations from a pre-determined groups with two or more variables recorded for each observation. It may be used as an ordination method, by displaying objects such that the discrimination among groups is maximized using the fewest dimensions as possible, or a classification method, by generating linear combination of variables that maximizes the probability of correctly assigning observations to groups. The aim is also to assess which variables most contribute to the discrimination among pre-determined groups.\n",
    "\n",
    "The two most common Discriminant Analysis methods are the Linear Discriminant Analysis (LDA) and the Quadratic Discriminant Analysis (QDA). LDA can only learn linear boundaries, while QDA can learn quadratic boundaries and is therefore more flexible. They are implemented respectively in `sklearn.discriminant_analysis.LinearDiscriminantAnalysis()` and `sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis()` functions of the the `sklearn` module. Here, we will give an example of Linear Discriminant Analysis\n",
    "\n",
    "#### 3.1 Linear Discriminant Analysis\n",
    "\n",
    "Import necessary modules and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define predictor and response variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df_scaled\n",
    "y = df_wine['quality'] # quality has 6 classes (3,4,5,6,7,8)\n",
    "\n",
    "#Fit the LDA model (we set to two components just for ilustration - default would be N_classes-1=5 in this case)\n",
    "model = LinearDiscriminantAnalysis(n_components=2)\n",
    "DLA = model.fit_transform(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some atributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight vectors of variables for each class\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of explained variance of the selected components\n",
    "model.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contribution of each variable (row) to each Component (column)\n",
    "model.scalings_ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the first two discriminant axis to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DLA_scores = pd.DataFrame(data = DLA, \n",
    "                            columns = ['LD1', 'LD2'])\n",
    "DLA_scores.head(6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define method to evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines the kfold crossvalidation settings for the next function 'cross_val_score'\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "\n",
    "#evaluate model (classification accuracy - from 0 to 1)\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fist discriminant plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.scatterplot(x=DLA_scores['LD1'],\n",
    "              y=DLA_scores['LD2'],\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.scatterplot(x=DLA_scores['LD1'],\n",
    "              y=DLA_scores['LD2'],\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "\n",
    "n = model.n_features_in_\n",
    "for i in range(11):\n",
    "        plt.arrow(0, 0, model.scalings_[i,0]*4, # Scalings were multiplied by a factor of 4 to just to facilitate the visualization\n",
    "                  model.scalings_[i,1]*4, # Scalings were multiplied by a factor of 4 to just to facilitate the visualization\n",
    "                  color = (0.1, 0.1, 0.1, 0.8),\n",
    "                  head_width=0.02) # plot arrows for each variable\n",
    "        plt.text(model.scalings_[i,0]* 4.3, # plot the names of the variables\n",
    "                 model.scalings_[i,1] * 4.3,\n",
    "                 list(df.columns)[i], \n",
    "                 color = (0.1, 0.1, 0.1, 0.8), \n",
    "                 ha = 'center', \n",
    "                 va = 'center') # variable labels for each arrow\n",
    "\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same plot but showing predictive classification areas (voronoi plane):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax0 = fig.add_subplot(111)\n",
    "ax0.set_xlim(-4,6)\n",
    "ax0.set_ylim(-7,4.2)\n",
    "\n",
    "colors = list(sns.color_palette(\"Spectral_r\", 6).as_hex())\n",
    "\n",
    "# Plot the voronoi spaces\n",
    "\n",
    "means = []\n",
    "for target in zip(np.unique(y)):\n",
    "    means.append(np.mean(DLA_scores[y==target],axis=0))\n",
    "    \n",
    "mesh_x, mesh_y = np.meshgrid(np.linspace(-4,6,200),np.linspace(-7,4.2,200)) # creates a grid of points to which we apply class predictions using the Linear discrimant model\n",
    "mesh = [] # creates an empty mesh\n",
    "\n",
    "for i in range(len(mesh_x)):\n",
    "    for j in range(len(mesh_x[0])):\n",
    "        date = [mesh_x[i][j],mesh_y[i][j]]\n",
    "        mesh.append((mesh_x[i][j],mesh_y[i][j]))\n",
    "\n",
    "NN = KNeighborsClassifier(n_neighbors=1) # set the k-nearest neighbor classifier to 1 nearest neighbor\n",
    "NN.fit(means,colors) # fit the 1 nearest neighbor\n",
    "predictions = NN.predict(np.array(mesh)) # Predict the class labels for the provided data (grid of points) \n",
    "ax0.scatter(np.array(mesh)[:,0],np.array(mesh)[:,1],color=predictions,alpha=0.1, linewidths=0 ) # plots the grid with the predicted class (color)\n",
    "\n",
    "\n",
    "sns.scatterplot(x=DLA_scores['LD1'],\n",
    "              y=DLA_scores['LD2'],\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              palette=sns.color_palette(\"Spectral_r\", 6),\n",
    "              linewidth=0.5,\n",
    "              edgecolor='black'\n",
    "              )\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "n = model.n_features_in_\n",
    "for i in range(11):\n",
    "        plt.arrow(0, 0, model.scalings_[i,0]*4, # Scalings were multiplied by a factor of 4 to just to facilitate the visualization\n",
    "                  model.scalings_[i,1]*4, # Scalings were multiplied by a factor of 4 to just to facilitate the visualization\n",
    "                  color = (0.1, 0.1, 0.1, 0.8),\n",
    "                  head_width=0.02) # plot arrows for each variable\n",
    "        plt.text(model.scalings_[i,0]* 4.3, # plot the names of the variables\n",
    "                 model.scalings_[i,1] * 4.3,\n",
    "                 list(df.columns)[i], \n",
    "                 color = (0.1, 0.1, 0.1, 0.8), \n",
    "                 ha = 'center', \n",
    "                 va = 'center') # variable labels for each arrow\n",
    "\n",
    "\n",
    "for target, c in zip(np.unique(y), colors):\n",
    "    ax0.scatter(np.mean(DLA_scores[y==target],axis=0)[0],\n",
    "                np.mean(DLA_scores[y==target],axis=0)[1],\n",
    "                c=c,s=150, linewidths=2, edgecolor='black'\n",
    "                )\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some boxplots of the variables that seems more correlated with the Discriminant axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, sharex=True, figsize=(6, 6))\n",
    "\n",
    "sns.boxplot(x='quality', y='alcohol', data = df_wine, ax=axs[0])\n",
    "sns.boxplot(x='quality', y='volatile acidity', data = df_wine, ax=axs[1])\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Running DA from scratch\n",
    "\n",
    "The code that follows computes the same LDA but without depending on any specific function.\n",
    "\n",
    "Step 0: Load in the data and split the descriptive and the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "y = df_wine['quality'] # quality has 6 classes (3,4,5,6,7,8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X.columns:\n",
    "    X[col] = StandardScaler().fit_transform(X[col].values.reshape(-1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2: Compute the mean vector mu and the mean vector per class mu_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(X,axis=0).values.reshape(11,1) # Mean vector mu --> Since the data has been standardized, the data means are zero \n",
    "\n",
    "mu_k = []\n",
    "for i, wine in enumerate(np.unique(df_wine['quality'])):\n",
    "    mu_k.append(np.mean(X.where(df_wine['quality']==wine),axis=0))\n",
    "mu_k = np.array(mu_k).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Compute the Scatter within and Scatter between matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_SW = []\n",
    "Nc = []\n",
    "for i,orchid in enumerate(np.unique(df_wine['quality'])):\n",
    "    a = np.array(X.where(df_wine['quality']==orchid).dropna().values-mu_k[:,i].reshape(1,11))\n",
    "    data_SW.append(np.dot(a.T,a))\n",
    "    Nc.append(np.sum(df_wine['quality']==orchid))\n",
    "SW = np.sum(data_SW,axis=0)\n",
    "\n",
    "SB = np.dot(Nc*np.array(mu_k-mu),np.array(mu_k-mu).T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Compute the Eigenvalues and Eigenvectors of SW^-1 SB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigval, eigvec = np.linalg.eig(np.dot(np.linalg.inv(SW),SB))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Select the two largest eigenvalues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_pairs = [[np.abs(eigval[i]),eigvec[:,i]] for i in range(len(eigval))]\n",
    "eigen_pairs = sorted(eigen_pairs,key=lambda k: k[0],reverse=True)\n",
    "w = np.hstack((eigen_pairs[0][1][:,np.newaxis].real,eigen_pairs[1][1][:,np.newaxis].real)) # Select two largest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Transform the data with Y=X*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X.dot(w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Plot 1st discriminant plane, showing predictive classification areas (voronoi plane)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax0 = fig.add_subplot(111)\n",
    "ax0.set_xlim(-6,4)\n",
    "ax0.set_ylim(-3,4)\n",
    "\n",
    "colors = list(sns.color_palette(\"Spectral_r\", 6).as_hex())\n",
    "\n",
    "# Plot the voronoi spaces\n",
    "\n",
    "means = []\n",
    "for target in zip(np.unique(y)):\n",
    "    means.append(np.mean(Y[y==target],axis=0))\n",
    "\n",
    "mesh_x, mesh_y = np.meshgrid(np.linspace(-6,4,200),np.linspace(-3,4,200)) # creates a grid of points to which we apply class predictions using the Linear discrimant model\n",
    "mesh = [] # creates an empty mesh\n",
    "\n",
    "for i in range(len(mesh_x)):\n",
    "    for j in range(len(mesh_x[0])):\n",
    "        date = [mesh_x[i][j],mesh_y[i][j]]\n",
    "        mesh.append((mesh_x[i][j],mesh_y[i][j]))\n",
    "\n",
    "NN = KNeighborsClassifier(n_neighbors=1) # set the k-nearest neighbor classifier to 1 nearest neighbor\n",
    "NN.fit(means,colors) # fit the 1 nearest neighbor\n",
    "predictions = NN.predict(np.array(mesh)) # Predict the class labels for the provided data (grid of points) \n",
    "ax0.scatter(np.array(mesh)[:,0],np.array(mesh)[:,1],color=predictions,alpha=0.05, linewidths=0 ) # plots the grid with the predicted class (color)\n",
    "\n",
    " \n",
    "# Plot the data\n",
    "for l,c in zip(np.unique(y),colors):\n",
    "    ax0.scatter(Y[0][y==l],\n",
    "                Y[1][y==l],\n",
    "               c=c, label=l, linewidths=0.5, edgecolor='black')\n",
    "ax0.legend(loc='upper right')\n",
    "\n",
    "# plot the means (centroids)\n",
    "means = []\n",
    "\n",
    "for target, c in zip(np.unique(y), colors):\n",
    "    ax0.scatter(np.mean(Y[y==target],axis=0)[0],\n",
    "                np.mean(Y[y==target],axis=0)[1],\n",
    "                c=c,s=150, linewidths=2, edgecolor='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/lda_qda.html\n",
    "\n",
    "https://statisticsglobe.com/biplot-pca-python\n",
    "\n",
    "https://python-course.eu/machine-learning/linear-discriminant-analysis-in-python.php\n",
    "\n",
    "https://www.mltut.com/linear-discriminant-analysis-python-complete-and-easy-guide/\n",
    "\n",
    "https://stackabuse.com/guide-to-multidimensional-scaling-in-python-with-scikit-learn/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

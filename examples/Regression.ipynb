{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Visualization of Complex Agro-Environmental Data\n",
    "---\n",
    "## Regression\n",
    "\n",
    "### 1. Run simple linear regression\n",
    "\n",
    "Simple linear regression is the simplest form of regression analysis. It is also commonly used in Exploratory Data Analysis when we are interested in exploring if a given continuous response variable is affected by other independent (or predictor) variables. When we aim at modelling a response continuous variable with multiple linear regression using a big set of potential candidate variables, simple regression analysis is often used as a first filter to select a subset of candidate variables. Again, significant effect of a predictor on the response variable does not imply causation; but causation implies a significant an effect of a predictor on the response. Along with correlation analysis, regression is also important as a basis to establish hypothesis to be tested with more elaborated confirmatory statistics.\n",
    "\n",
    "Linear regression models may be run with several python modules such as SciPy, statsmodel and scikit-learn.\n",
    "\n",
    "##### Example with the [penguin](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# import the packages we are going to be using\n",
    "import numpy as np # for getting our distribution\n",
    "import pandas as pd # to handle data frames\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import seaborn as sns # for plotting\n",
    "from scipy import stats # to compute statistics\n",
    "\n",
    "# import data ('penguin' dataset)\n",
    "data = pd.read_csv('penguins_lter.csv')\n",
    "data.drop(data.iloc[:,14:18], axis=1, inplace=True)\n",
    "data = data.dropna()\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's regress the variable 'body mass' against 'culmen depth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `penguin` dataset to relate body mass as function of culmen depth (i.e. to infer body mass from culmen depth)\n",
    "\n",
    "x=data[\"Culmen Depth (mm)\"]\n",
    "y=data[\"Body Mass (g)\"]\n",
    "\n",
    "# Execute a method that returns some important key values of Linear Regression:\n",
    "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
    "\n",
    "# plot data with fitted line\n",
    "def myfunc(x):\n",
    "  return intercept + slope * x # function that returns fitted values\n",
    "\n",
    "mymodel = list(map(myfunc, x)) # apply function to each x value\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, mymodel, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overal relationship tends to be negative. Now we will test the null hypothesis that the slope of relationship is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the regression test\n",
    "print('slope estimate=%.2f, r-square=%.3f, p=%.6f' % (slope, r**2, p))\n",
    "alpha=0.05\n",
    "if p <= alpha:\n",
    " print('reject H0 that the slope of the relationship is = 0')\n",
    "else:\n",
    " print('fail to reject H0 that the slope of the relationship is = 0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **statsmodel** module provides a more complete output using much less code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "x2 = x\n",
    "x2 = sm.add_constant(x2) # adding a constant (Intercept)\n",
    "\n",
    "model = sm.OLS(y, x2).fit()\n",
    "predictions = model.predict(x2) \n",
    "\n",
    "print_model = model.summary()\n",
    "print(print_model)\n",
    "\n",
    "# Some notes on model outputs:\n",
    "\n",
    "# Omnibus tests are a kind of statistical test. They test whether the explained variance in a \n",
    "# set of data is significantly greater than the unexplained variance, overall. One example is the F-test in the analysis of variance.\n",
    "\n",
    "# The Durbin Watson statistic is a test for autocorrelation in a regression model's output. \n",
    "# The statistic ranges from zero to four, with a value of 2.0 indicating zero autocorrelation.\n",
    "\n",
    "# Jarqueâ€“Bera test is a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution.\n",
    "\n",
    "# The adjusted r-square adjusts the value of R2 to avoid overestimating the impact of adding independent variables to the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other modules that have implemented regression analysis, such as the **sklearn** module:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the scatter plot above, it seems that the data is grouped, which are probably related to the different species present in the dataset. Let's check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same scatterplot but showing data per species\n",
    "sns.scatterplot(x=x, y=y, hue=data[\"Species\"])\n",
    "plt.plot(x, mymodel)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We confirmed that the groups correspond to species. We also can see that the relationship within each group is positive, and not negative, as the overall data suggests. We can plot the regression line for each group running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot by group\n",
    "sns.lmplot(x='Culmen Depth (mm)', y='Body Mass (g)', hue=\"Species\", data=data)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each group, the relationship is clearly positive, despite an overall negative relationship. This paradox is called the `Simpson's Paradox` (nothing to do with the Simpson's series), as described by Simpson (1951).\n",
    "\n",
    "It is therefore more appropriate to run the regression to each species separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run same regression but only for the species 'versicolor'\n",
    "\n",
    "x=data[(data['Species']=='Adelie Penguin (Pygoscelis adeliae)')]['Culmen Depth (mm)']\n",
    "y=data[(data['Species']=='Adelie Penguin (Pygoscelis adeliae)')]['Body Mass (g)']\n",
    "\n",
    "# Execute a method that returns some important key values of Linear Regression:\n",
    "slope, intercept, r, p, std_err = stats.linregress(x, y)\n",
    "\n",
    "# plot data with fitted line\n",
    "def myfunc(x):\n",
    "  return intercept + slope * x # function that returns fitted values\n",
    "\n",
    "mymodel = list(map(myfunc, x)) # apply function to each x value\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, mymodel, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model output table\n",
    "x = sm.add_constant(x) # adding a constant (Intercept)\n",
    "\n",
    "model = sm.OLS(y, x).fit()\n",
    "predictions = model.predict(x) \n",
    "\n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run multiple linear regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running multiple linear regression, the model will contain more than one predictor variable. The single effect of a variable may change in the presence of other variables. In the following example we will use the `Penguin` dataset to regress the body mass as a function of the remaining variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data['Body Mass (g)']\n",
    "x=data[[\"Culmen Depth (mm)\", \"Culmen Length (mm)\", \"Flipper Length (mm)\"]]\n",
    "\n",
    "x = sm.add_constant(x) # adding a constant (Intercept)\n",
    "\n",
    "model = sm.OLS(y, x).fit()\n",
    "predictions = model.predict(x) \n",
    "\n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also use the formula interface of statsmodels (like in R) to compute regression with multiple predictors. We just need append the predictors to the formula via a '+' symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import formula api as alias smf \n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# to apply a formula the header names cannot have spaces - need to change the header names\n",
    "data.rename(columns={'Body Mass (g)':'body_mass'}, inplace=True) # inplace=\"True\" means that df will be updated\n",
    "data.rename(columns={'Culmen Depth (mm)':'culmen_depth'}, inplace=True) \n",
    "data.rename(columns={'Culmen Length (mm)':'culmen_length'}, inplace=True) \n",
    "data.rename(columns={'Flipper Length (mm)':'flipper_length'}, inplace=True)\n",
    "\n",
    "# formula: response ~ predictor1 + predictor2 + ...\n",
    "model = smf.ols(formula='body_mass ~ culmen_depth + culmen_length + flipper_length', data=data).fit()\n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now plot the partial response (or dependence) plots, which represents the effect of each variable in the presence of the remaining. This effect may be very different from the single effect, i.e., when running simple linear regression with each predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_partregress_grid(model)\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart below shows how the single effect of 'culmen depth' is very different from its joint effect with the remaining variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula: response ~ predictor\n",
    "model2 = smf.ols(formula='body_mass ~ culmen_depth', data=data).fit()\n",
    "fig = sm.graphics.plot_partregress_grid(model2)\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next chart uses the plot_fit function to plot the fitted values versus a chosen independent variable. It includes prediction confidence intervals and optionally plots the true dependent variable. Let's check the fit plot for each predictor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_fit(model, \"culmen_depth\")\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_fit(model, \"culmen_length\")\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.plot_fit(model, \"flipper_length\")\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important step when running regression is to perform a regression diagnostic to check that the assumptions are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sm.graphics.influence_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic plots\n",
    "# Influence plot - points with high leverage are extreme values that potentially may show a high influence on the fitted response\n",
    "fig = sm.graphics.influence_plot(model, criterion=\"cooks\") # criterion defines the size of the bubble. Cook's distance shows the influence of each observation on the fitted response values\n",
    "fig.tight_layout(pad=1.0)\n",
    "# point 169 has both a high leverage and cook's distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *statsmodel* has no specific function to run the most common diagnostic plots, but it provides a code to generate such function. Check below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to produce functions to run diagnostic plots  \n",
    "# https://www.statsmodels.org/dev/examples/notebooks/generated/linear_regression_diagnostics_plots.html\n",
    "\n",
    "# base code\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from statsmodels.tools.tools import maybe_unwrap_results\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Type\n",
    "import statsmodels\n",
    "\n",
    "style_talk = 'seaborn-talk'    #refer to plt.style.available\n",
    "\n",
    "class Linear_Reg_Diagnostic():\n",
    "    \"\"\"\n",
    "    Diagnostic plots to identify potential problems in a linear regression fit.\n",
    "    Mainly,\n",
    "        a. non-linearity of data\n",
    "        b. Correlation of error terms\n",
    "        c. non-constant variance\n",
    "        d. outliers\n",
    "        e. high-leverage points\n",
    "        f. collinearity\n",
    "\n",
    "    Author:\n",
    "        Prajwal Kafle (p33ajkafle@gmail.com, where 3 = r)\n",
    "        Does not come with any sort of warranty.\n",
    "        Please test the code one your end before using.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 results: Type[statsmodels.regression.linear_model.RegressionResultsWrapper]) -> None:\n",
    "        \"\"\"\n",
    "        For a linear regression model, generates following diagnostic plots:\n",
    "\n",
    "        a. residual\n",
    "        b. qq\n",
    "        c. scale location and\n",
    "        d. leverage\n",
    "\n",
    "        and a table\n",
    "\n",
    "        e. vif\n",
    "\n",
    "        Args:\n",
    "            results (Type[statsmodels.regression.linear_model.RegressionResultsWrapper]):\n",
    "                must be instance of statsmodels.regression.linear_model object\n",
    "\n",
    "        Raises:\n",
    "            TypeError: if instance does not belong to above object\n",
    "\n",
    "        Example:\n",
    "        >>> import numpy as np\n",
    "        >>> import pandas as pd\n",
    "        >>> import statsmodels.formula.api as smf\n",
    "        >>> x = np.linspace(-np.pi, np.pi, 100)\n",
    "        >>> y = 3*x + 8 + np.random.normal(0,1, 100)\n",
    "        >>> df = pd.DataFrame({'x':x, 'y':y})\n",
    "        >>> res = smf.ols(formula= \"y ~ x\", data=df).fit()\n",
    "        >>> cls = Linear_Reg_Diagnostic(res)\n",
    "        >>> cls(plot_context=\"seaborn-paper\")\n",
    "\n",
    "        In case you do not need all plots you can also independently make an individual plot/table\n",
    "        in following ways\n",
    "\n",
    "        >>> cls = Linear_Reg_Diagnostic(res)\n",
    "        >>> cls.residual_plot()\n",
    "        >>> cls.qq_plot()\n",
    "        >>> cls.scale_location_plot()\n",
    "        >>> cls.leverage_plot()\n",
    "        >>> cls.vif_table()\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(results, statsmodels.regression.linear_model.RegressionResultsWrapper) is False:\n",
    "            raise TypeError(\"result must be instance of statsmodels.regression.linear_model.RegressionResultsWrapper object\")\n",
    "\n",
    "        self.results = maybe_unwrap_results(results)\n",
    "\n",
    "        self.y_true = self.results.model.endog\n",
    "        self.y_predict = self.results.fittedvalues\n",
    "        self.xvar = self.results.model.exog\n",
    "        self.xvar_names = self.results.model.exog_names\n",
    "\n",
    "        self.residual = np.array(self.results.resid)\n",
    "        influence = self.results.get_influence()\n",
    "        self.residual_norm = influence.resid_studentized_internal\n",
    "        self.leverage = influence.hat_matrix_diag\n",
    "        self.cooks_distance = influence.cooks_distance[0]\n",
    "        self.nparams = len(self.results.params)\n",
    "\n",
    "\n",
    "    def __call__(self, plot_context='seaborn-paper'):\n",
    "        # print(plt.style.available)\n",
    "        with plt.style.context(plot_context):\n",
    "            fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "            self.residual_plot(ax=ax[0,0])\n",
    "            self.qq_plot(ax=ax[0,1])\n",
    "            self.scale_location_plot(ax=ax[1,0])\n",
    "            self.leverage_plot(ax=ax[1,1])\n",
    "            plt.show()\n",
    "\n",
    "        self.vif_table()\n",
    "        return fig, ax\n",
    "\n",
    "\n",
    "\n",
    "    def residual_plot(self, ax=None):\n",
    "        \"\"\"\n",
    "        Residual vs Fitted Plot\n",
    "\n",
    "        Graphical tool to identify non-linearity.\n",
    "        (Roughly) Horizontal red line is an indicator that the residual has a linear pattern\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        sns.residplot(\n",
    "            x=self.y_predict,\n",
    "            y=self.residual,\n",
    "            lowess=True,\n",
    "            scatter_kws={'alpha': 0.5},\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8},\n",
    "            ax=ax)\n",
    "\n",
    "        # annotations\n",
    "        residual_abs = np.abs(self.residual)\n",
    "        abs_resid = np.flip(np.sort(residual_abs))\n",
    "        abs_resid_top_3 = abs_resid[:3]\n",
    "        for i, _ in enumerate(abs_resid_top_3):\n",
    "            ax.annotate(\n",
    "                i,\n",
    "                xy=(self.y_predict[i], self.residual[i]),\n",
    "                color='C3')\n",
    "\n",
    "        ax.set_title('Residuals vs Fitted', fontweight=\"bold\")\n",
    "        ax.set_xlabel('Fitted values')\n",
    "        ax.set_ylabel('Residuals')\n",
    "        return ax\n",
    "\n",
    "\n",
    "    def qq_plot(self, ax=None):\n",
    "        \"\"\"\n",
    "        Standarized Residual vs Theoretical Quantile plot\n",
    "\n",
    "        Used to visually check if residuals are normally distributed.\n",
    "        Points spread along the diagonal line will suggest so.\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        QQ = ProbPlot(self.residual_norm)\n",
    "        QQ.qqplot(line='45', alpha=0.5, lw=1, ax=ax)\n",
    "\n",
    "        # annotations\n",
    "        abs_norm_resid = np.flip(np.argsort(np.abs(self.residual_norm)), 0)\n",
    "        abs_norm_resid_top_3 = abs_norm_resid[:3]\n",
    "        for r, i in enumerate(abs_norm_resid_top_3):\n",
    "            ax.annotate(\n",
    "                i,\n",
    "                xy=(np.flip(QQ.theoretical_quantiles, 0)[r], self.residual_norm[i]),\n",
    "                ha='right', color='C3')\n",
    "\n",
    "        ax.set_title('Normal Q-Q', fontweight=\"bold\")\n",
    "        ax.set_xlabel('Theoretical Quantiles')\n",
    "        ax.set_ylabel('Standardized Residuals')\n",
    "        return ax\n",
    "\n",
    "    def scale_location_plot(self, ax=None):\n",
    "        \"\"\"\n",
    "        Sqrt(Standarized Residual) vs Fitted values plot\n",
    "\n",
    "        Used to check homoscedasticity of the residuals.\n",
    "        Horizontal line will suggest so.\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        residual_norm_abs_sqrt = np.sqrt(np.abs(self.residual_norm))\n",
    "\n",
    "        ax.scatter(self.y_predict, residual_norm_abs_sqrt, alpha=0.5);\n",
    "        sns.regplot(\n",
    "            x=self.y_predict,\n",
    "            y=residual_norm_abs_sqrt,\n",
    "            scatter=False, ci=False,\n",
    "            lowess=True,\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8},\n",
    "            ax=ax)\n",
    "\n",
    "        # annotations\n",
    "        abs_sq_norm_resid = np.flip(np.argsort(residual_norm_abs_sqrt), 0)\n",
    "        abs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3]\n",
    "        for i in abs_sq_norm_resid_top_3:\n",
    "            ax.annotate(\n",
    "                i,\n",
    "                xy=(self.y_predict[i], residual_norm_abs_sqrt[i]),\n",
    "                color='C3')\n",
    "        ax.set_title('Scale-Location', fontweight=\"bold\")\n",
    "        ax.set_xlabel('Fitted values')\n",
    "        ax.set_ylabel(r'$\\sqrt{|\\mathrm{Standardized\\ Residuals}|}$');\n",
    "        return ax\n",
    "\n",
    "    def leverage_plot(self, ax=None):\n",
    "        \"\"\"\n",
    "        Residual vs Leverage plot\n",
    "\n",
    "        Points falling outside Cook's distance curves are considered observation that can sway the fit\n",
    "        aka are influential.\n",
    "        Good to have none outside the curves.\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        ax.scatter(\n",
    "            self.leverage,\n",
    "            self.residual_norm,\n",
    "            alpha=0.5);\n",
    "\n",
    "        sns.regplot(\n",
    "            x=self.leverage,\n",
    "            y=self.residual_norm,\n",
    "            scatter=False,\n",
    "            ci=False,\n",
    "            lowess=True,\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8},\n",
    "            ax=ax)\n",
    "\n",
    "        # annotations\n",
    "        leverage_top_3 = np.flip(np.argsort(self.cooks_distance), 0)[:3]\n",
    "        for i in leverage_top_3:\n",
    "            ax.annotate(\n",
    "                i,\n",
    "                xy=(self.leverage[i], self.residual_norm[i]),\n",
    "                color = 'C3')\n",
    "\n",
    "        xtemp, ytemp = self.__cooks_dist_line(0.5) # 0.5 line\n",
    "        ax.plot(xtemp, ytemp, label=\"Cook's distance\", lw=1, ls='--', color='red')\n",
    "        xtemp, ytemp = self.__cooks_dist_line(1) # 1 line\n",
    "        ax.plot(xtemp, ytemp, lw=1, ls='--', color='red')\n",
    "\n",
    "        ax.set_xlim(0, max(self.leverage)+0.01)\n",
    "        ax.set_title('Residuals vs Leverage', fontweight=\"bold\")\n",
    "        ax.set_xlabel('Leverage')\n",
    "        ax.set_ylabel('Standardized Residuals')\n",
    "        ax.legend(loc='upper right')\n",
    "        return ax\n",
    "\n",
    "    def vif_table(self):\n",
    "        \"\"\"\n",
    "        VIF table\n",
    "\n",
    "        VIF, the variance inflation factor, is a measure of multicollinearity.\n",
    "        VIF > 5 for a variable indicates that it is highly collinear with the\n",
    "        other input variables.\n",
    "        \"\"\"\n",
    "        vif_df = pd.DataFrame()\n",
    "        vif_df[\"Features\"] = self.xvar_names\n",
    "        vif_df[\"VIF Factor\"] = [variance_inflation_factor(self.xvar, i) for i in range(self.xvar.shape[1])]\n",
    "\n",
    "        print(vif_df\n",
    "                .sort_values(\"VIF Factor\")\n",
    "                .round(2))\n",
    "\n",
    "\n",
    "    def __cooks_dist_line(self, factor):\n",
    "        \"\"\"\n",
    "        Helper function for plotting Cook's distance curves\n",
    "        \"\"\"\n",
    "        p = self.nparams\n",
    "        formula = lambda x: np.sqrt((factor * p * (1 - x)) / x)\n",
    "        x = np.linspace(0.001, max(self.leverage), 50)\n",
    "        y = formula(x)\n",
    "        return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the diagnostic plots to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run diagnostic plots\n",
    "cls = Linear_Reg_Diagnostic(model)\n",
    "fig, ax = cls()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Linear regression diagnostics https://www.statsmodels.org/dev/examples/notebooks/generated/linear_regression_diagnostics_plots.html\n",
    "\n",
    "Ordinary Least Squares https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html\n",
    "\n",
    "Regression Plots https://www.statsmodels.org/dev/examples/notebooks/generated/regression_plots.html\n",
    "\n",
    " Simpson, Edward H. (1951). The Interpretation of Interaction in Contingency Tables. *Journal of the Royal Statistical Society*, Series B. 13: 238â€“241."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
